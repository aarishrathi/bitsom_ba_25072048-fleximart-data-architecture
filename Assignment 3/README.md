# FlexiMart Data Architecture Project

**Student Name:** Aarish Rathi  
**Student ID:** bitsom_ba_25072048  
**Email:** aarishrathiwork@gmail.com  
**Date:** December 24, 2025

## Project Overview

This project implements a comprehensive data architecture solution for FlexiMart, an e-commerce platform, covering three critical aspects of modern data management. Part 1 builds an ETL pipeline that extracts raw CSV data, performs data quality transformations (handling duplicates, missing values, and format standardization), and loads clean data into a normalized MySQL database. Part 2 explores NoSQL solutions using MongoDB to address schema flexibility challenges with diverse product catalogs, demonstrating document-oriented data storage and aggregation capabilities. Part 3 designs and implements a star schema data warehouse optimized for analytics, enabling efficient OLAP queries for business intelligence including drill-down analysis, product performance metrics, and customer segmentation insights.

## Repository Structure

```
bitsom_ba_25072048-fleximart-data-architecture/
│
├── README.md                           # Root documentation
├── .gitignore                          # Ignore unnecessary files
│
├── data/                               # Input data files (provided)
│   ├── customers_raw.csv
│   ├── products_raw.csv
│   └── sales_raw.csv
│
├── part1-database-etl/
│   ├── README.md                       # Part 1 overview
│   ├── etl_pipeline.py
│   ├── schema_documentation.md
│   ├── business_queries.sql
│   ├── data_quality_report.txt         # Generated by ETL script
│   └── requirements.txt                # Python dependencies
│
├── part2-nosql/
│   ├── README.md                       # Part 2 overview
│   ├── nosql_analysis.md
│   ├── mongodb_operations.js
│   └── products_catalog.json
│
└── part3-datawarehouse/
    ├── README.md                       # Part 3 overview
    ├── star_schema_design.md
    ├── warehouse_schema.sql
    ├── warehouse_data.sql
    └── analytics_queries.sql
```

## Technologies Used

- Python 3.x, pandas, mysql-connector-python
- MySQL 8.0 / PostgreSQL 14
- MongoDB 6.0

## Setup Instructions

### Database Setup

```bash
# Create databases
mysql -u root -p -e "CREATE DATABASE fleximart;"
mysql -u root -p -e "CREATE DATABASE fleximart_dw;"

# Run Part 1 - ETL Pipeline
python part1-database-etl/etl_pipeline.py

# Run Part 1 - Business Queries
mysql -u root -p fleximart < part1-database-etl/business_queries.sql

# Run Part 3 - Data Warehouse
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_schema.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_data.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/analytics_queries.sql
```

### MongoDB Setup

```bash
mongosh < part2-nosql/mongodb_operations.js
```

## Key Learnings

This project provided deep insights into the end-to-end data architecture lifecycle, from raw data ingestion to analytical reporting. I learned how to design robust ETL pipelines that handle real-world data quality issues like duplicate records, missing values, and inconsistent date formats, implementing proper error handling and data quality metrics tracking. The NoSQL component highlighted the trade-offs between relational and document-oriented databases, demonstrating how MongoDB's flexible schema solves the "sparse matrix" problem for diverse product catalogs while maintaining query performance through embedded documents. The data warehouse design taught me the importance of dimensional modeling with star schemas, where fact tables store measurable business events and dimension tables provide descriptive context, enabling fast analytical queries through denormalized structures optimized for OLAP operations.

## Challenges Faced

1. **Handling Multiple Date Formats in ETL Pipeline:** The raw sales data contained dates in various formats (YYYY-MM-DD, DD/MM/YYYY, MM-DD-YYYY), causing parsing failures. I solved this by implementing a robust multi-format date parsing strategy that tries multiple formats sequentially, ensuring maximum data retention while maintaining data quality standards. This required careful handling of nullable vs non-nullable date fields according to schema constraints.

2. **Designing Efficient Star Schema for Analytics:** Creating an effective star schema required balancing query performance with storage efficiency. The challenge was determining the appropriate grain for the fact table (transaction-level vs order-level) and deciding which attributes belonged in dimensions versus the fact table. I addressed this by analyzing the analytical query requirements first, then designing dimensions with appropriate hierarchies (date dimension with year-quarter-month-day levels) and ensuring the fact table grain matched the business questions to minimize aggregation complexity.

